{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = pd.read_csv('non_rt_english_unique_id_with_text.csv', usecols=['id', 'user_id', 'text'], dtype={'id':'str', 'user_id':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(\"parsed_non_rt_english_sm.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "with open(\"parsed_non_rt_english_lg.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import spacy_transformers\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(7)\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "with open(\"parsed_non_rt_english_trf.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']))), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = pd.read_csv('non_rt_french_unique_id.csv', usecols=['id', 'user_id', 'text'], dtype={'id':'str', 'user_id':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "with open(\"parsed_non_rt_french_sm.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('fr_core_news_lg')\n",
    "\n",
    "with open(\"parsed_non_rt_french_lg.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import spacy_transformers\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(7)\n",
    "\n",
    "nlp = spacy.load('fr_dep_news_trf')\n",
    "\n",
    "with open(\"parsed_non_rt_french_trf.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']))), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv('non_rt_german_unique_id.csv', usecols=['id', 'user_id', 'text'], dtype={'id':'str', 'user_id':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "with open(\"parsed_non_rt_german_sm.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "with open(\"parsed_non_rt_german_lg.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']), n_process=5)), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import spacy_transformers\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(7)\n",
    "\n",
    "nlp = spacy.load('de_dep_news_trf')\n",
    "\n",
    "with open(\"parsed_non_rt_german_trf.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"user_id\", \"sent_i\", \"token_i\", \"text\", \"lemmas\", \"like_url\", \"pos\", \"tag\", \"ents\", \"deps\", \"head\", \"head_i\",\n",
    "                     \"head_pos\", \"children\", \"children_i\"])\n",
    "    for id, user_id, d in tqdm(zip(docs[\"id\"], docs['user_id'], nlp.pipe(list(docs['text']))), total=len(docs)):\n",
    "        for sent_i, sent in enumerate(d.sents):\n",
    "            for t in sent:\n",
    "                try:\n",
    "                    writer.writerow([id, user_id, sent_i, t.i, t.text, t.lemma_, t.like_url, t.pos_, t.tag_, t.ent_type_, t.dep_, t.head, t.head.i,\n",
    "                                     t.head.pos_, \"|\".join(str(c) for c in t.children), \"|\".join(str(c.i) for c in t.children)])\n",
    "                except TypeError:\n",
    "                    print()\n",
    "                    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatGPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
